{
    "contents" : "\\documentclass{article}\n\\usepackage{fancyvrb}\n%%% joe\n\\usepackage{natbib}\n%\\usepackage[natbib=true,style=authoryear,maxcitenames=6]{biblatex}\n%\\usepackage{verbatim}\n\n\\bibliographystyle{asa}\n%%  joe\n\n\\usepackage{geometry}\n%\\usepackage{extsizes}\n\\geometry{letterpaper, portrait, margin=0.5in}\n\\usepackage{graphicx}\n\\usepackage{amssymb,amsmath}\n\\usepackage{float}\n\\usepackage{multicol}\n\\usepackage{booktabs}\n\\usepackage{caption}\n\\usepackage{subcaption}\n\\usepackage{arydshln}\n\\begin{document}\n\\setkeys{Gin}{width=0.8\\textwidth}\n\\SweaveOpts{concordance=TRUE}\n\\title{What's new with the SMVCIR \\\\ R package}\n\\author{Dan McGuire}\n\\maketitle\n\\newpage\n\n\\citet{smc08} developed the visualization procedure sliced mean variance covariance inverse regression (SMVCIR).\nThis is a form of discriminant analysis which can be used as both a classification and a dimension reduction\ntechnique.  The advantage of the SMVCIR algorithm and projecting your original data coordinates into the SMVCIR discriminant space is that SMVCIR coordinates seek to take into account variance and covariance differences between classes as well as mean differences between classes.  \\citet{lsm14} developed  a dimensionality test, which provides the user with the minimum and optimal number of discriminant dimensions needed for the dataset.\n\nOriginally used as a graphical group discrimination procedure, new additions to the SMVCIR R package include formal prediction methods as well as extensions to the original graphing functions from the package.  Changes to original package along with code examples of new functions and simulations for classification accuracy and area under the ROC curve for several datasets are included in the sections to follow.\n\\section*{Changes to the main SMVCIR function}\nDirect discriminant coordinates estimates, dimension correlation matrices, and coefficients for the model are all suppressed from popping up when the function is run.  They can be accessed after the model is built through objects called \\texttt{direct}, \\texttt{dimCorr}, and \\texttt{coefficients} respectively.\nPlot outputs are also suppressed from the main function.  Plots can be called as desired through the function \\texttt{plotcoords()} detailed later, and an optional scree plot based on the dimensionality test can be called in the main function if the argument \\texttt{scree\\_plot=TRUE}.  Another small change is that class labels are retained throughout the model building process, and can be seen when accessing direct coordinate estimates as well as in plot labels.\\\\\n\\textbf{Summary of model objects:}\n\\begin{itemize}\n\\item \\texttt{groups} number of classes\n\\item \\texttt{predictors} number of covariates from original dataset\n\\item \\texttt{statdim} Number of dimensions chosen by SMVCIR dimensionality test.  If test==FALSE when model is built, this is NA.\n\\item \\texttt{sighatz} Pooled covariance matrix of discriminant coordinates.\n\\item \\texttt{sighatx} Covariance matrix of original variables.\n\\item \\texttt{groupindex} Column number of classification variable in original dataset.\n\\item \\texttt{class.props} Sample proportion of observations in training set belonging to each class.\n\\item \\texttt{muhat\\_ls} Marginal class means after original data has been centered and scaled.\n\\item \\texttt{muhat\\_z}  Marginal class means after original data has been transformed to smvcir discriminant coordinates.\n\\item \\texttt{dimCorr}  Correlation matrix of smvcir dimensions.\n\\item \\texttt{maxTRANScm} Maximum correlation between smvcir dimensions.\n\\item \\texttt{coefficients}  smvcir coefficients.  Can be accessed by base \\texttt{coef()} function.\n\\item \\texttt{direct}  These are the direct estimates of discriminant coordinates for the data that the model was trained on.\n\\item \\texttt{spansetF} Spanning set estimate used to build smvcir kernel.\n\\item \\texttt{originalx}  This is the original centered and scaled dataset.  Needed for the \\texttt{predict.smvcir()} method.\n\\end{itemize}\n\\newpage\nBelow is an example of how the main function \\texttt{smvcir()} may be used, this is from the Wisconsin Breast Cancer dataset from the UCI Machine learning repository \\citet{uciWisc}.\n<<>>=\nlibrary(SMVCIR)\nlibrary(caret)\ndata(wisc)\nset.seed(12)\ntrain<-createDataPartition(wisc$tumor, p = .8, list = FALSE)    ####Get rows for training dataset\nmodel<-smvcir(group = \"tumor\", data = wisc[train,], test = TRUE)####Create smvcir model object on training set\n\nsummary(model)\n@\n\n\n\\section*{Predict Methods}\n\nThe new \\texttt{predict.smvcir()} method can give formal class and probability predictions based on mahalanobis distance from nearest class centroid.  For example, using our model built on the Wisconsin Breast Cancer dataset we want to predict the observations on our test set not used to train the model.  Observations are projected into the SMVCIR discriminant space based on the previously built model, then we assign observations to the class centroid which they are nearest to based on mahalanobis distance.  Below, we choose our maximum dimension (\\texttt{maxdim}) to be the recommended dimension based on the dimensionality test for the previously built model.  The default will use the full possible set of $p$ dimensions.\n<<>>=\nclass_preds<-predict(model = model, newdata = wisc[-train,], type = \"class\", maxdim = model$statdim)\nconfusionMatrix(class_preds, wisc$tumor[-train], positive = \"malignant\")\n@\nBy classifying observations based on distance from nearest centroid, we have overall accuracy of 94.81\\% on our test set.\n\nWe can also get probability estimates that an observation belongs to a particular class.  Given that mahalanobis distance is chi-square distributed with $p$ degrees of freedom.  The distance from each centroid is calculated, a chi-square probability is produced, then probabilities of each class are combined in a way that lets them sum to 1.  We assess the likelihood of belonging to a particular class in terms of the probability of belonging to all classes \\cite{dfaURL}.  For example, based on chi-squared distance we find an observation has a probability of 0.7 belonging to Class 2, and 0.9 of belonging to Class 1, and 0.5 of belonging to Class 3.  The observation is closest to Class 1's centroid, followed by Class 2 then Class 3.  We calculate $P(Class 1) = \\frac{0.9}{0.9 + 0.7 + 0.5} = 0.4286$, \\\\\n$P(Class 2) = \\frac{0.7}{0.9 + 0.7 + 0.5} = 0.333$, and\\\\\n$P(Class 3) = \\frac{0.5}{0.9 + 0.7 + 0.5} = 0.238$.\\\\\n\nBelow is an example of how probability estimates may be produced using our Wisconsin Breast Cancer test set.\n<<>>=\nprob_preds<-predict(model = model, newdata = wisc[-train,], type = \"prob\", maxdim = model$statdim)\nhead(prob_preds)\n@\n\n\n\nWe may also wish to only project our original data into the SMVCIR discriminant space, and then perform some other classification techniques, or visualize our new coordinates with a plot.  This can be achieved with the same function.\n<<>>=\nnew_coords<-predict(model = model, newdata = wisc[-train,], coordinates_only = TRUE, maxdim = model$statdim)\nnew_coords$tumor<-wisc[-train,]$tumor\nhead(new_coords)\n@\nNow that we have SMVCIR coordinates and a reduced number of dimensions, we could apply some other classification technique.  We could build a random forest \\citet{rfPackage}and predict our test set, for example, using the below code:\n<<>>=\nlibrary(randomForest)\nrfMod<-randomForest(tumor~., data = model$direct[,c(1:5, 10)])\np2<-predict(rfMod, newdata = new_coords)\nconfusionMatrix(p2, new_coords$tumor, positive = \"malignant\")\n@\n\n\n\\section*{Plot Methods}\nA few changes have been made to the original plotting machine.  The new function \\texttt{plotcoords()} takes either model objects of class ``smvcir\" or a data frame of new predicted coordinates, and plots the SMVCIR coordinates in up to three dimensions.  One new feature is that in two or three dimensions, a support vector machine model can now be built and visualized.  Given that SMVCIR seeks to give separation between classes, visualizing an svm model with discriminant coordinates can be a useful technique in evaluating the effectiveness of SMVCIR in separating the class groups.\n\n\\subsection*{2-Dimensional plots}\n\nThe below code produces 2-dimensional plots of the first five dimensions of SMVCIR space, taking as input our model object ``model\".  Using the e1071 package \\citet{e1071Package}, a support vector machine (svm) model is built using the dimensions and kernel specified by the user, and then visualized according to the pairwise dimensions using a filled contour.  In this example, we choose to plot the first five dimensions, so the support vector machine is uses these five dimensions as covariates. The grey area is where our support vector machine chooses to classify a tumor as ``malignant\" in this case, and we can see that most of the discrimination from an svm model can be seen with respect to the first dimension.  A linear kernel is chosen for the svm.\n\n<<>>=\nplotcoords(model, dimensions = 1:5, build_svm = TRUE, kernel = \"linear\")\n@\n\\begin{center}\n\\includegraphics[width=0.4\\textwidth]{2dsmv}\n\\end{center}\nWe may note, however, that the above plot illustrates an svm along with the same data that was used to build it.  What if we wanted to plot testing data on this svm model?  First we build the model on our training data using the stored model object \\texttt{direct}, containing the ``direct\" estimates of our discriminant coordinates.  We're careful to build the model using the same dimensions that we will afterwards pass to \\texttt{plotcoords()}.  We then pass the model to \\texttt{plotcoords()} via the \\texttt{svmModel} argument while plotting the predicted coordinates from our test dataset.\n<<>>=\nsvm.Model<-svm(tumor~., data = model$direct[,c(1:5, 10)], type = \"C-classification\", kernel = \"linear\")\nplotcoords(new_coords, dimensions = 1:5, build_svm = TRUE, svmModel = svm.Model)\n@\n\\begin{center}\n\\includegraphics[width=0.4\\textwidth]{test2dsmv}\n\\end{center}\nHere we have our test data set plotted against the svm model built from our training data.\n\n\n\\subsection*{3 dimensional plots}\nWhen we use option \\texttt{GL=TRUE}, a 3-dimensional plot can be produced calling the \\texttt{rgl} package \\citet{rglPkg}.  This can give an excellent visualization as to how well the data is separated by projecting onto smvcir coordinate space, and these 3-d plots can also be rotated.  The below code produces the following plot of the first three dimensions of our breast cancer model.  As before, using the option \\texttt{build\\_svm = TRUE} builds a support vector machine with the kernel and dimensions specified using \\texttt{misc3d} package \\citet{misc3dPkg}, in this case a radial kernel on the first three dimension.  We can see in the plot that most of the variation can be explained in the first dimension.  While malignant tumors are all over the place, benign tumors are comparably well restricted between 0 and 2 in the first dimension.\n<<eval=FALSE>>=\nplotcoords(model, dimensions = 1:3, build_svm = TRUE, kernel = \"radial\", GL = TRUE)\n@\n\\begin{center}\n\\includegraphics[width=0.4\\textwidth]{3dsmv}\n\\end{center}\nLets take a look at this same model along with our test data set.\n<<eval=FALSE>>=\nsvm.Mod2<-svm(tumor~., data=model$direct[,c(1:3, 10)], type = \"C-classification\", kernel = \"radial\")\nplotcoords(new_coords, 1:3, kernel = \"radial\", build_svm = TRUE, GL = TRUE, svmModel = svm.Mod2)\n@\n\\begin{center}\n\\includegraphics[width=0.4\\textwidth]{test3dsvm}\n\\end{center}\n\n\n\n\\section*{Dimension reduction case study:  Pima Indians Diabetes Dataset}\nDimension reduction is large motivator for using SMVCIR.  We saw earlier using the Wisconsin Breast Cancer data set that although there are originally nine covariates, we effectively summarize the data with five.\nIn the below example we use the Pima Indian diabetes dataset from UCI Machine Learning Repository \\citet{uciPima}.\nwhile we originally have nine covariates, our model chooses \\textit{two} discriminant dimensions to be optimal.  We see nearly identical performance to that of logistic regression and random forest models with respect to area under the ROC curve \\citet{pROCpkg}.\n<<>>=\nlibrary(pROC)\nlibrary(MASS)\nset.seed(529)\ntrain<-createDataPartition(pima$diabetes, p = .8, list = FALSE)\nmod<-smvcir(group = \"diabetes\", data = pima[train,], test = TRUE)\nmod$statdim\n\nprob_preds<-predict(model = mod, newdata = pima[-train,], type = \"prob\", maxdim = 2)\nrocCurve<-roc(pima[-train,]$diabetes, prob_preds[,2])\nlogitmod<-glm(diabetes~., data = pima[train,], family = \"binomial\")\nrfmod<-randomForest(diabetes~., data = pima[train,])\nrfpred<-predict(rfmod, newdata = pima[-train,], type = \"prob\")\nlppred<-predict(logitmod, newdata = pima[-train,], type = \"response\")\n\nrocLogistic<-roc(pima[-train,]$diabetes, lppred)\nrocRf<-roc(pima[-train,]$diabetes, rfpred[,2])\nplot(rocCurve, col = \"steelblue\", las = 1, main = \"ROC Curves \\nfor Three Classification Methods\")\nplot(rocLogistic, add = TRUE, col = \"indianred\")\nplot(rocRf, add = TRUE, col = \"green\")\nlegend(\"bottomright\", c(\"SMVCIR\", \"logistic regression\", \"Random Forest\"), col = c(\"steelblue\", \"indianred\", \"green\"), lty = 1, lwd = 2)\n@\n\\begin{center}\n\\includegraphics[width=0.4\\textwidth]{rocPlot}\n\\end{center}\n\n\n\n\\section*{Comparison of bootstrapped estimates of classification accuracy and Area Under ROC curve for three datasets}\n\n\nThe goal of the following simulation was to evaluate SMVCIR classification accuracy, as well as the effectiveness of the dimensionality test.  The following algorithm compares SMVCIR classification accuracy by nearest-centroid distance with that of logistic regression and support vector machine models built from SMVCIR coordinates, as well as with models built from original data coordinates.\nThe algorithm utilized the following steps:\\\\\n\\textbf{Step 1.} Take sample of observations of size \\texttt{nrow(dataset)} with replacement from original dataset.\\\\\n\\textbf{Step 2.}Using \\texttt{smvcir} discriminant coordinates with full possible number of dimensions: \\\\\nBuild nearest centroid, logistic regression, and support vector machine models.\\\\\n\\textbf{Step 3.}Using \\texttt{smvcir} discriminant coordinates with \\textit{recommended} number of dimensions based on dimensionality test:\\\\\nBuild nearest centroid, logistic regression, and support vector machine models.\\\\\n\\textbf{Step 4.} Using original dataset: \\\\\nBuild logistic regression, and support vector machine models.\\\\\n\\textbf{Step 5.} Use these models to predict on the ``out of bag samples\" \\textit{not} used to build the model.  Collect classification accuracy and area under ROC curve (AUC) statistics for each model.\\\\\n\\textbf{Step 6.} Repeat ``B\" times.\n\\\\\\\\\n\\textbf{Final Step:}Average accuracy and AUC results from each of the eight possible methods.\n\\\\\\\\\nThis was done with the \\texttt{bootit} function found in the appendix.\\\\\n\\textbf{Results}:\\\\\nSeveral notable observations from our simulations:\n\nThe dimensionality test, which tests the amount of SMVCIR dimensions needed for discriminating between classes seems to work well.  In the case of the Wisconsin Breast Cancer dataset, predictions based on the reduced number of dimensions actually improve classification performance over using the full set of $p$ possible dimensions.  For the Pima and Banknote datasets, classification is only marginally worse than other methods.  Overall classification accuracy and AUC results appear to be highly similar between both type of model (nearest-centroid, logistic regression, and support vector machine) as well as coordinates used to build the model (SMVCIR reduced dimensions, SMVCIR full dimensions, and original data coordinates).  The largest advantage to the SMVCIR algorithm likely lies in its ability to discriminate between classes in a reduced number of dimensions.  For datasets with highly heterogeneous marginal class covariance matrices such as the Banknote dataset, modest improvements in both accuracy and AUC can be found by using the SMVCIR algorithm.\n\n**Also of note is that for all three datasets, logistic regression models built with the full set of $p$ dimensions appear to have identitical performance to logistic regression models built from the original data coordinates.\n\n\n    \\begin{table}\n    \\caption{Average Accuracy for 20 bootstrap simulations Wisconsin Breast Cancer Data}\n\t\t\\begin{tabular}{c | c | r | c}\n\t\t                               & SMVCIR w Dimensionality test & SMVCIR Full Dimensions       & Original Coordinates  \\\\\n\t\t\\hline\n\t\tNearest Centroid               &  0.9597692                    & 0.959215                       & \\\\\n\t\tLogistic Regression            &  0.9611824                    & 0.9582281                      & 0.9582281\\\\\n\t\tSupport Vector Machine         &  0.9581735                    & 0.9553565                      & 0.9614831\n         \\end{tabular}\n    \\end{table}\n\n\n\n        \\begin{table}\n    \\caption{Average AUC for 20 bootstrap simulations Wisconsin Breast Cancer Data}\n\t\t\\begin{tabular}{c | c | r | c}\n\t\t                               & SMVCIR w Dimensionality test & SMVCIR Full Dimensions       & Original Coordinates  \\\\\n\t\t\\hline\n\t\tNearest Centroid               &  0.9854760                    & 0.9761520                        & \\\\\n\t\tLogistic Regression            &  0.9929513                    & 0.9927271                      & 0.9927271\\\\\n\t\tSupport Vector Machine         &  0.9879737                   & 0.9892754                      & 0.9876604\n         \\end{tabular}\n    \\end{table}\n\n        \\begin{table}\n    \\caption{Average Accuracy for 20 bootstrap simulations Pima Diabetes Data}\n\t\t\\begin{tabular}{c | c | r | c}\n\t\t                               & SMVCIR w/ Dimensionality test & SMVCIR Full Dimensions       & Original Coordinates  \\\\\n\t\t\\hline\n\t\tNearest Centroid               &  0.7356803                    & 0.7470875                      & \\\\\n\t\tLogistic Regression            &  0.7510413                    &  0.7588068                      &  0.7588068\\\\\n\t\tSupport Vector Machine         &   0.7404241                    & 0.7398190                      & 0.7412006\n         \\end{tabular}\n    \\end{table}\n\n            \\begin{table}\n    \\caption{Average AUC for 20 bootstrap simulations Pima Diabetes Data}\n\t\t\\begin{tabular}{c | c | r | c}\n\t\t                               & SMVCIR w Dimensionality test & SMVCIR Full Dimensions       & Original Coordinates  \\\\\n\t\t\\hline\n\t\tNearest Centroid               &  0.8145613                    &  0.8066819                       & \\\\\n\t\tLogistic Regression            &   0.8195818                     & 0.8237954                      & 0.8237954\\\\\n\t\tSupport Vector Machine         &  0.7847283                   & 0.8071310                      & 0.8090777\n         \\end{tabular}\n    \\end{table}\n\n        \\begin{table}\n    \\caption{Average Accuracy for 20 bootstrap simulations Banknote Data}\n\t\t\\begin{tabular}{c | c | r | c}\n\t\t                               & SMVCIR w/ Dimensionality test & SMVCIR Full Dimensions       & Original Coordinates  \\\\\n\t\t\\hline\n\t\tNearest Centroid               &  0.9880221                    &  0.9949850                      & \\\\\n\t\tLogistic Regression            &  0.9880328                    &  0.9823498                      &  0.9823498\\\\\n\t\tSupport Vector Machine         &   0.9865793                    &  0.9789067                      & 0.9894376\n         \\end{tabular}\n    \\end{table}\n\n                \\begin{table}\n    \\caption{Average AUC for 20 bootstrap simulations Banknote Data}\n\t\t\\begin{tabular}{c | c | r | c}\n\t\t                               & SMVCIR w Dimensionality test & SMVCIR Full Dimensions       & Original Coordinates  \\\\\n\t\t\\hline\n\t\tNearest Centroid               &  0.9996557                    &  0.9998713                       & \\\\\n\t\tLogistic Regression            &   0.9953389                     & 0.9984412                     & 0.9984412\\\\\n\t\tSupport Vector Machine         &  0.9983909                  & 0.9978727                      & 0.9986883\n         \\end{tabular}\n    \\end{table}\n\\newpage\n\\section*{Appendix}\n\\texttt{bootit} function used to estimate accuracy:\n<<eval= FALSE>>=\n\n@\n<<>>=\nlibrary(pROC)\nlibrary(e1071)\nbootit<-function(boots = 10, seed = sample(1:10000, size = 1), groupcol, dat){\n  set.seed(seed)\n  acc_mat<-matrix(NA, boots,8)\n  auc_mat<-matrix(NA, boots,8)\n\n  for(i in 1:boots){\n    ####centroid smvcir method\n    datsamp<-sample(1:nrow(dat), replace = TRUE)\n    mod<-smvcir(group = names(dat)[groupcol], data = dat[datsamp,], test = TRUE)\n    oob_class_preds<-predict(mod, newdata = dat[-datsamp,], type = \"class\")\n    oob_prob_preds<-predict(mod, newdata = dat[-datsamp,], type = \"prob\")\n    tab<-table(oob_class_preds, dat[-datsamp,groupcol])\n    acc_mat[i,1]<-sum(diag(tab))/sum(tab) ###centroid oob overall accuracy\n    roc1<-roc(dat[-datsamp,groupcol], oob_prob_preds[,2])\n    auc_mat[i,1]<-auc(roc1)\n    ####logistic smvcir method\n    pred_coords<-predict(mod, newdata = dat, coordinates_only = TRUE)\n    pred_coords[,groupcol]<-dat[,groupcol]\n    names(pred_coords)[groupcol]<-names(dat)[groupcol]\n    logistic_mod<-glm(as.formula(paste(names(pred_coords)[groupcol],\"~.\")), data = pred_coords[datsamp,],\n                      family = \"binomial\")\n    glm_preds<-predict(logistic_mod, newdata = pred_coords[-datsamp,], type = \"response\")\n    classp<-ifelse(glm_preds < .5, levels(pred_coords[,groupcol])[1], levels(pred_coords[,groupcol])[2])\n    tab2<-table(classp, pred_coords[-datsamp,groupcol])\n    roc2<-roc(pred_coords[-datsamp,groupcol], glm_preds)\n    auc_mat[i, 2]<-auc(roc2)\n    acc_mat[i, 2]<-sum(diag(tab2))/sum(tab2)\n    ####svm method\n    svmMod<-svm(as.formula(paste(names(pred_coords)[groupcol],\"~.\")), data = pred_coords[datsamp,],\n                probability = TRUE)\n    ppred<-predict(svmMod, newdata = pred_coords[-datsamp,], probability = TRUE)\n    tab3<-table(ppred, pred_coords[-datsamp,groupcol])\n    roc3<-roc(pred_coords[-datsamp,groupcol], attr(ppred, \"probabilities\")[,2])\n    auc_mat[i, 3]<-auc(roc3)\n    acc_mat[i, 3]<-sum(diag(tab3))/sum(tab3)\n    #####WITH OPTIMAL DIMENSIONS SMVCIR\n    ########\n    ####centroid smvcir method\n    oob_class_preds<-predict(mod, newdata = dat[-datsamp,], type = \"class\", maxdim = mod$statdim)\n    oob_prob_preds<-predict(mod, newdata = dat[-datsamp,], type = \"prob\", maxdim = mod$statdim)\n    tab<-table(oob_class_preds, dat[-datsamp,groupcol])\n    acc_mat[i,4]<-sum(diag(tab))/sum(tab) ###centroid oob overall accuracy\n    roc1<-roc(dat[-datsamp,groupcol], oob_prob_preds[,2])\n    auc_mat[i,4]<-auc(roc1)\n    ####logistic smvcir method\n    pred_coords<-predict(mod, newdata = dat, coordinates_only = TRUE, maxdim = mod$statdim)\n    ngc<-(ncol(pred_coords)+1)\n    pred_coords[,ngc]<-dat[,groupcol]\n    names(pred_coords)[ngc]<-names(dat)[groupcol]\n    logistic_mod<-glm(as.formula(paste(names(pred_coords)[ngc],\"~.\")), data = pred_coords[datsamp,],\n                      family = \"binomial\")\n    glm_preds<-predict(logistic_mod, newdata = pred_coords[-datsamp,], type = \"response\")\n    classp<-ifelse(glm_preds < .5, levels(pred_coords[,ngc])[1], levels(pred_coords[,ngc])[2])\n    tab2<-table(classp, pred_coords[-datsamp,ngc])\n    roc2<-roc(pred_coords[-datsamp,ngc], glm_preds)\n    auc_mat[i, 5]<-auc(roc2)\n    acc_mat[i, 5]<-sum(diag(tab2))/sum(tab2)\n    ####svm method\n    svmMod<-svm(as.formula(paste(names(pred_coords)[ngc],\"~.\")), data = pred_coords[datsamp,],\n                probability = TRUE)\n    ppred<-predict(svmMod, newdata = pred_coords[-datsamp,], probability = TRUE)\n    tab3<-table(ppred, pred_coords[-datsamp,ngc])\n    roc3<-roc(pred_coords[-datsamp,ngc], attr(ppred, \"probabilities\")[,2])\n    auc_mat[i, 6]<-auc(roc3)\n    acc_mat[i, 6]<-sum(diag(tab3))/sum(tab3)\n    #######WITH ORIGINAL COORDINATES\n    #####logistic original coordinates\n    dat[,-groupcol]<-scale(dat[-groupcol], center = TRUE, scale = TRUE)\n    ####Advantage of centering and scaling data\n    ####for svm used later\n    logistic_mod2<-glm(as.formula(paste(names(dat)[groupcol],\"~.\")),data = dat[datsamp,],\n                       family = \"binomial\")\n    glm_preds2<-predict(logistic_mod2, newdata = dat[-datsamp,], type = \"response\")\n    classp2<-ifelse(glm_preds2 < .5,levels(dat[,groupcol])[1], levels(dat[,groupcol])[2])\n    tab22<-table(classp2, dat[-datsamp,groupcol])\n    roc22<-roc(dat[-datsamp,groupcol], glm_preds2)\n    auc_mat[i, 7]<-auc(roc22)\n    acc_mat[i, 7]<-sum(diag(tab22))/sum(tab22)\n    ####\n    ####svm original coordinates\n    svmMod2<-svm(as.formula(paste(names(dat)[groupcol],\"~.\")), data = dat[datsamp,],\n                 probability = TRUE)\n    ppred2<-predict(svmMod2, newdata = dat[-datsamp,], probability = TRUE)\n    tab32<-table(ppred2, dat[-datsamp,groupcol])\n    roc32<-roc(dat[-datsamp,groupcol], attr(ppred2, \"probabilities\")[,2])\n    auc_mat[i, 8]<-auc(roc32)\n    acc_mat[i, 8]<-sum(diag(tab32))/sum(tab32)\n  }\n  #####\n  colnames(auc_mat)<-c(\"centroid\", \"logistic (smvcir)\", \"svm (smvcir)\", \"reducedCentroid\",\n                       \"reducedlog\", \"reducedsvm\", \"logistic\", \"svm\")\n  colnames(acc_mat)<-colnames(auc_mat)\n  return(list(acc_mat = acc_mat, auc_mat = auc_mat, seed = seed))\n\n}\n\n@\nAccuracy results in above tables can be replicated by using the following:\n<<eval=FALSE>>=\nwiscboot<-bootit(boots = 20, seed = 411, dat = wisc, groupcol = 10)\npimboot<-bootit(boots = 20, seed = 411, dat = pima, groupcol = 9)\nbankboot<-bootit(boots = 20, seed = 411, dat = banknote, groupcol = 7)\n@\n\n%% joe\n\\bibliography{refs}\n%%\n\n\n\\end{document}\n",
    "created" : 1464060125350.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1477102208",
    "id" : "B8CBCE14",
    "lastKnownWriteTime" : 1468877570,
    "path" : "C:/Users/Dan/Desktop/smvcir_writeup/sweave.Rnw",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 9,
    "source_on_save" : false,
    "type" : "sweave"
}